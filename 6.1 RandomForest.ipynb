{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose and Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose:-\n",
    "\n",
    "- To fit a model on 'Dataset_model'\n",
    "- Dataset: created by isolating first policy bought by a customer (using policy_owner_number and RCD)\n",
    "\n",
    "### Observations:-\n",
    "\n",
    "#### Main conclusion:-\n",
    "- Final iteration\n",
    "    - variables used: 'premium', 'afyp', 'sum_assured', 'Owner_salary', 'city', 'CUST_prod_cat', 'DSTNAME', 'STATNAME', 'age',\n",
    "           'Own_Edu', 'Occupation',\n",
    "            'contract_type',\n",
    "            'City_classification',\n",
    "            'channel_flag',\n",
    "            'Product_Club_Manual',\n",
    "           'Policy_term'\n",
    "    - accuracy on training data: 0.9998\n",
    "    - accuracy on testing data: 0.9392\n",
    "    - AUC: 0.792\n",
    "    - Confusion matrix (testing data only): pre defined threshold\n",
    "        - True positives: 57.07\n",
    "        - True negatives: 94.48\n",
    "        - False positives: 42.93\n",
    "        - False negatives: 5.52\n",
    "\n",
    "- Fine tuning\n",
    "    - accuracy on testing data: 93.47\n",
    "    - Confusion matrix (testing data only): threshold value = 0.37\n",
    "        - True positives: 46.57\n",
    "        - True negatives: 95.15\n",
    "        - False positives: 53.43\n",
    "        - False negatives: 4.85\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset used Dataset_model: Merged cleaned, continuous variables imputed with mean, rest recordes with NaN dropped. then isolate records for first RCD, groupedby policy_owner_number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "dataset = pd.read_csv(\"Dataset_model.csv\")\n",
    "\n",
    "#convert RCD to datetime\n",
    "dataset[\"RCD\"] = pd.to_datetime(dataset[\"RCD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Stats for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tweak to show full values in describe instead of in exp terms\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "#describe and info\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(dataset.head())\n",
    "    display(dataset.describe())\n",
    "    display(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping unneeded variables\n",
    "- Dropping columns\n",
    "    - 'Own_Education': keeping Own_Edu\n",
    "    - 'own_occupation: keeping Occ_Profile, Occupation_Group, Occupation\n",
    "    - 'policy_number', 'policy_owner_number': Identifiers\n",
    "    - 'RCD': Datetime\n",
    "    - 'Freq': used to obtain target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['Own_Education', 'own_occupation',\n",
    "              'policy_number', 'policy_owner_number', \n",
    "              'RCD', \n",
    "              'Freq'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert floats to int\n",
    "floatlist = list(dataset.select_dtypes('float').columns)\n",
    "\n",
    "for col in floatlist:\n",
    "    dataset.loc[:,col] = dataset.loc[:,col].apply(np.ceil)\n",
    "    if dataset.loc[:,col].isna().sum() == 0:\n",
    "        dataset[col] = dataset[col].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelencode for categorical vars\n",
    "\n",
    "#import library\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#create list of categorical vars\n",
    "catcolsm = list(dataset.select_dtypes('object').columns)\n",
    "\n",
    "#encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#function to encode\n",
    "def labelencode(data, col):\n",
    "    nonulls=np.array(data.dropna())\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    impute_ordinal = le.fit_transform(impute_reshape)\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    globals()[col+'_map'] = dict(zip(range(len(le.classes_)), le.classes_))\n",
    "    return data\n",
    "\n",
    "#labelencode\n",
    "for col in catcolsm:\n",
    "    labelencode(dataset[col], col)\n",
    "    \n",
    "#review results\n",
    "display(dataset.head())\n",
    "\n",
    "#display dictionaries created.\n",
    "for col in catcolsm:\n",
    "    display(globals()[col+'_map'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert object dtype to category dtype\n",
    "for col in catcolsm:\n",
    "    dataset[col] = dataset[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seperating target variable\n",
    "X=dataset.drop(['target'],axis=1)\n",
    "y=dataset[['target']]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier: Iteration 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. train test split, feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "#took 5-6 mins to run\n",
    "\n",
    "x_train, x_test, y_train,  y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "rf=RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking feature importances again using RandomForestClassifier\n",
    "\n",
    "plt.bar([x for x in range(len(rf.feature_importances_))], rf.feature_importances_)\n",
    "plt.show()\n",
    "\n",
    "display(list(zip(x_test, rf.feature_importances_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imp variables from RandomForestClassifier** (> or close to 0.05)\n",
    "- premium, afyp, sum_assured, Owner_salary, city, CUST_prod_cat, DSTNAME, STATNAME, age: having high importance\n",
    "- Own_Edu, Occupation: moderate importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check fit on train data\n",
    "\n",
    "y_pred_train=rf.predict(x_train)\n",
    "print(\"Accuracy against the training data: \", metrics.accuracy_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy on Test Data\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "print(\"Accuracy against test data: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AUC, ROC, Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_probs = [0 for _ in range(len(y_test))]\n",
    "rf_probs = rf.predict_proba(x_test)\n",
    "\n",
    "#Note: r_auc refers to the worst case model possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "r_auc = roc_auc_score(y_test, r_probs) \n",
    "rf_auc = roc_auc_score(y_test, rf_probs[:, 1]) \n",
    "# We use [:,1] since predict.proba gives values in column 0 as probs for value 0, in column 1: probs for value 1\n",
    "\n",
    "print('Random classifier: AUROC = %.3f' % (r_auc))\n",
    "print('RandomForestClassifier: AUROC = %.3f' % (rf_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fpr, rf_tpr, rf_threshold = roc_curve(y_test, rf_probs[:, 1])\n",
    "r_fpr, r_tpr, r_threshold = roc_curve(y_test, r_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "plt.plot(r_fpr, r_tpr, linestyle = '--', label = 'Random prediction (AUROC = %0.3f)' % r_auc)\n",
    "plt.plot(rf_fpr, rf_tpr, linestyle = '--', label = 'Random prediction (AUROC = %0.3f)' % rf_auc)\n",
    "\n",
    "#title\n",
    "plt.title('ROC plot')\n",
    "\n",
    "#labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "#legend\n",
    "plt.legend()\n",
    "\n",
    "#show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(y_test, y_pred), columns = [0,1] , index = [0,1])\n",
    "display(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display((cm/np.sum(cm))*100)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap (cm/np.sum(cm), annot = True, fmt = '.2%', cmap = 'Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "false negatives: 5.76 to 10% ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier: Iteration 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only important features \n",
    "feature importance close to 0.05 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating target variable\n",
    "X2=dataset[['premium', 'afyp', 'sum_assured', 'Owner_salary', 'city', 'CUST_prod_cat', 'DSTNAME', 'STATNAME', 'age',\n",
    "           'Own_Edu', 'Occupation',\n",
    "            'contract_type',\n",
    "            'City_classification',\n",
    "            'channel_flag',\n",
    "            'Product_Club_Manual',\n",
    "           'Policy_term']]\n",
    "y2=dataset[['target']]\n",
    "X2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. train test split, feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "#took 5-6 mins to run\n",
    "\n",
    "x_train2, x_test2, y_train2,  y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=0)\n",
    "rf=RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking feature importances again using RandomForestClassifier\n",
    "\n",
    "plt.bar([x for x in range(len(rf.feature_importances_))], rf.feature_importances_)\n",
    "plt.show()\n",
    "\n",
    "display(list(zip(x_test2, rf.feature_importances_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partial dependence plots?\n",
    "skater?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check fit on train data\n",
    "\n",
    "y_pred_train2=rf.predict(x_train2)\n",
    "print(\"Accuracy against the training data: \", metrics.accuracy_score(y_train2, y_pred_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy on Test Data\n",
    "\n",
    "y_pred2 = rf.predict(x_test2)\n",
    "print(\"Accuracy against test data: \", metrics.accuracy_score(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AUC, ROC, Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_probs2 = [0 for _ in range(len(y_test2))]\n",
    "rf_probs2 = rf.predict_proba(x_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_auc2 = roc_auc_score(y_test2, r_probs2) \n",
    "rf_auc2 = roc_auc_score(y_test2, rf_probs2[:, 1]) \n",
    "# We use [:,1] since predict.proba gives values in column 0 as probs for value 0, in column 1: probs for value 1\n",
    "\n",
    "print('Random classifier: AUROC = %.3f' % (r_auc2))\n",
    "print('RandomForestClassifier: AUROC = %.3f' % (rf_auc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fpr2, rf_tpr2, rf_threshold2 = roc_curve(y_test2, rf_probs2[:, 1])\n",
    "r_fpr2, r_tpr2, r_threshold2 = roc_curve(y_test2, r_probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "plt.plot(r_fpr2, r_tpr2, linestyle = '--', label = 'Random prediction (AUROC = %0.3f)' % r_auc2)\n",
    "plt.plot(rf_fpr2, rf_tpr2, linestyle = '--', label = 'Random prediction (AUROC = %0.3f)' % rf_auc2)\n",
    "\n",
    "#title\n",
    "plt.title('ROC plot')\n",
    "\n",
    "#labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "#legend\n",
    "plt.legend()\n",
    "\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = pd.DataFrame(confusion_matrix(y_test2, y_pred2), columns = [0,1] , index = [0,1])\n",
    "display(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display((cm2/np.sum(cm2))*100)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap (cm2/np.sum(cm2), annot = True, fmt = '.2%', cmap = 'Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Predict with new threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set threshold\n",
    "\n",
    "threshold = 0.37\n",
    "\n",
    "#create column predicted based on threshold\n",
    "\n",
    "predicted = (rf_probs2[:,1] >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy against test data: \", metrics.accuracy_score(y_test2, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create confusion matrix again\n",
    "cm3 = pd.DataFrame(confusion_matrix(y_test2, predicted), columns = [0,1] , index = [0,1])\n",
    "display(cm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display((cm3/np.sum(cm3))*100)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap (cm3/np.sum(cm3), annot = True, fmt = '.2%', cmap = 'Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
