{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose and Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "   - to read data from two datasets: 'CustSegList' and 'merged'\n",
    "   - compare them to find relationships between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "  \n",
    "### Main conclusion:\n",
    "   - Thus, Merged dataset is wrapped up data of CustSegList on 'policy_owner_number'\n",
    "   - Rather than a simple groupby, this is done by assessing individual columns differently\n",
    "   - Categorical Variables:\n",
    "        - these contain Identity Information which stays same across both datasets\n",
    "    - Continuous Variables :\n",
    "         - Any Identity Information stays same, eg. Owner salary\n",
    "         - Variables related to Insurance policies change: \n",
    "             - sum_assured and afyp are added. \n",
    "             - PPT, policy term, billing frequency, max of two values is chosen\n",
    "             - premium, smaller of the values is chosen for majority rows\n",
    "    - DateTime:\n",
    "         - DOB stays same\n",
    "         - RCD, smaller of the values is chosen for majority rows\n",
    "         - RCD for range for both datasets: 2013-01-01 to 2017-12-03\n",
    "  \n",
    " ***Proceed on further analysis with 'merged' dataset***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import datasets\n",
    "df_csl = pd.read_csv('CustSegList.csv')\n",
    "df_merged = pd.read_csv('merged.csv')\n",
    "\n",
    "#Marital_status, Own_Edu, Occupation_Group fix\n",
    "df_merged.replace(['N', 'N.A', 'MISSING'], np.nan, inplace=True )\n",
    "df_csl.replace(['N', 'N.A', 'MISSING'], np.nan, inplace=True )\n",
    "\n",
    "df_csl.replace({'multi_cust': np.nan}, 'N', inplace = True)\n",
    "\n",
    "#STATNAME fix and Focus_region fix\n",
    "\n",
    "objectlistm = list(df_merged.select_dtypes('object').columns)\n",
    "for col in objectlistm:\n",
    "    df_merged[col] = df_merged[col].str.upper()\n",
    "\n",
    "objectlistc = list(df_csl.select_dtypes('object').columns)\n",
    "for col in objectlistc:\n",
    "    df_csl[col] = df_csl[col].str.upper()\n",
    "\n",
    "# putting them under SOUTH\n",
    "df_csl['Focus_region'].replace(['KKG','ANDHRA','TAMIL NADU'], 'SOUTH', inplace = True)\n",
    "df_merged['Focus_region'].replace(['KKG','ANDHRA','TAMIL NADU'], 'SOUTH', inplace = True)\n",
    "\n",
    "# RCD and LA_DOB fix\n",
    "df_merged[\"LA_DOB\"]= pd.to_datetime(df_merged[\"LA_DOB\"])\n",
    "df_merged[\"RCD\"] = pd.to_datetime(df_merged[\"RCD\"])\n",
    "\n",
    "df_csl[\"LA_DOB\"]= pd.to_datetime(df_csl[\"LA_DOB\"])\n",
    "df_csl[\"RCD\"] = pd.to_datetime(df_csl[\"RCD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#observe data\n",
    "display(df_merged.head())\n",
    "display(df_csl.head())\n",
    "\n",
    "\n",
    "#describe data\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "display(df_merged.describe())\n",
    "display(df_csl.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find unique Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking differences\n",
    "print(\"csl shape:\",df_csl.shape, \"\\n\",  \n",
    "      \"merged shape:\", df_merged.shape, \"\\n\",\n",
    "      \"rows difference between csl and merged:\", abs(df_csl.shape[0]-df_merged.shape[0]), \"\\n\"\n",
    "      \"column difference between csl and merged:\", abs(df_csl.shape[1]-df_merged.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking ID number match\n",
    "print(\"How many policy_owner_number entries of csl are present in merged: \",df_csl.loc[:,'policy_owner_number'].isin(df_merged.loc[:,'policy_owner_number']).sum(), \"\\n\"\n",
    "     \"How many policy_number entries of csl are present in merged: \",df_csl.loc[:,'policy_number'].isin(df_merged.loc[:,'policy_number']).sum(), \"\\n\"\n",
    "     \"How many Unnamed:0 entries of csl are present in merged : \",df_csl.loc[:,\"Unnamed: 0\"].isin(df_merged.loc[:,\"Unnamed: 0\"]).sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations: \n",
    "- Total number of records for csl is 750598, same as number observed for entries of csl in merged.\n",
    "- Confirmed with two candidates for identifiers: Policy Owner Number and Policy Number.\n",
    "- Thus All records in CSL are present in Merged dataset.\n",
    "- Unnamed: 0 can be dropped.\n",
    "- Checking further now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkin duplicate values for ID\n",
    "def check_duplicates(df, col):\n",
    "    return abs(len(set(df.loc[:, col])) - len(df.loc[:, col]))\n",
    "\n",
    "print(\"policy owner number\".center(30,'-'),\n",
    "      \"csl: {}\".format(check_duplicates(df_csl, \"policy_owner_number\")), \n",
    "      \"merged: {}\".format(check_duplicates(df_merged,\"policy_owner_number\")), sep=\"\\n\")\n",
    "\n",
    "print(\"policy number\".center(30,'-'),\n",
    "      \"csl: {}\".format(check_duplicates(df_csl, \"policy_number\")), \n",
    "      \"merged: {}\".format(check_duplicates(df_merged,\"policy_number\")), sep=\"\\n\")\n",
    "\n",
    "print(\"Unnamed: 0 :\".center(30,'-'),\n",
    "      \"csl: {}\".format(check_duplicates(df_csl, \"Unnamed: 0\")), \n",
    "      \"merged: {}\".format(check_duplicates(df_merged,\"Unnamed: 0\")), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: \n",
    "- Duplicates only present for policy_owner_number in 'merged' dataset.\n",
    "- The number of these duplicate values equals the difference between rows of merged and csl datasets\n",
    "- Thus we can say that policy_number is unique identifier across both merged and csl\n",
    "- And, policy_owner_number is unique identifier only for csl. Confirmed by the fact that the column, combined_policy contains multiple numbers which correspond to policy_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Unnamed:0\n",
    "x= \"Unnamed: 0\"\n",
    "df_csl.drop([x], axis=1, inplace=True)\n",
    "df_merged.drop([x], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset df_diff which has rows present in merged, but not in csl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list with uncommon values of policy number in two datasets\n",
    "x= set(df_merged['policy_number'])-set(df_csl['policy_number'])\n",
    "print(len(x)) #matches with the difference betn record numbers found earlier\n",
    "l1= sorted(list(x))\n",
    "\n",
    "#create new dataframe\n",
    "df_diff= df_merged.loc[df_merged['policy_number'].isin(l1)]\n",
    "df_diff.reset_index(inplace=True)\n",
    "df_diff.drop(['index'], axis=1, inplace=True)\n",
    "df_diff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Merged and CSL datasets on common identifier( policy_number)\n",
    "\n",
    "We compare every datapoint by\n",
    "- selecting common policy_numbers in csl and merged\n",
    "- selecting common columns in csl and merged\n",
    "- comparing every datapoint by function get_disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common columns\n",
    "com = sorted(list(set.intersection(set(df_csl.columns), set(df_merged.columns))))\n",
    "print(com, \"\\n\")\n",
    "\n",
    "#uncommon columns in merged\n",
    "unc_merged= sorted(list(set(df_merged.columns)-set.intersection(set(df_csl.columns), set(df_merged.columns))))\n",
    "print(unc_merged, \"\\n\")\n",
    "\n",
    "#uncommon columns in csl\n",
    "unc_csl= sorted(list(set(df_csl.columns)-set.intersection(set(df_csl.columns), set(df_merged.columns))))\n",
    "print(unc_csl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disparities(df1, df2):\n",
    "    #common columns\n",
    "    common = sorted(list(set.intersection(set(df1.columns), set(df2.columns))))\n",
    "    \n",
    "    #create dataframes\n",
    "    global dfm1\n",
    "    dfm1 = df1.loc[df1['policy_number'].isin(df2['policy_number']), common]\n",
    "    global dfc1\n",
    "    dfc1 = df2.loc[df2['policy_number'].isin(df1['policy_number']), common]\n",
    "    \n",
    "    #resetting index for new dataframe: essential for comparing\n",
    "    dfm1.reset_index(inplace=True)\n",
    "    dfm1.drop(['index'], axis=1, inplace=True)\n",
    "    dfc1.reset_index(inplace=True)\n",
    "    dfc1.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    #create new df for checking\n",
    "    global df_check\n",
    "    df_check = pd.DataFrame(columns=common)\n",
    "\n",
    "    \n",
    "    #Comparison\n",
    "    #False indicates no mismatch\n",
    "    for x in common:\n",
    "        df_check[x]= np.where(dfm1[x] == dfc1[x], False, True)\n",
    "    \n",
    "    #value counts\n",
    "    print(\"False indicates no mismatch\")\n",
    "    for col in df_check.columns: # can also call common instead of df_check.columns\n",
    "        print(col.center(50,'*'), \"\\n\", df_check[col].value_counts(), \"\\n\")\n",
    "    \n",
    "    #make dictionary to locate disparities\n",
    "    global location\n",
    "    location= {}\n",
    "    for i in df_check.index.values:\n",
    "        for j in df_check:\n",
    "            if df_check.loc[i,j] == True:\n",
    "                if i in location.keys():\n",
    "                    location[i].append(j)\n",
    "                else:\n",
    "                    location[i] = []\n",
    "                    location[i].append(j)\n",
    "    \n",
    "    # Get variables with disparities\n",
    "    l3= []\n",
    "    for val in location.values():\n",
    "        for i in val:\n",
    "            l3.append(i)\n",
    "\n",
    "\n",
    "    global dis_var \n",
    "    dis_var = list(set(l3))\n",
    "    \n",
    "    \n",
    "    #getting index numbers with disparities\n",
    "    global dis_ind \n",
    "    dis_ind = list(location.keys())\n",
    "    \n",
    "    print(\"variables with disparity:\", len(dis_var))\n",
    "    print(\"rows with disparity (unique from total disparities which tells about individual datapoints,i.e cells in csv, with mismatch\", len(dis_ind))\n",
    "    #uncomment below line of code to export newly created file\n",
    "    \n",
    "    \"\"\"\n",
    "    #create a merged dataframe to check them\n",
    "    disparity = pd.merge(left=dfm1.loc[dis_ind, ['policy_number']+dis_var], right=dfc1.loc[dis_ind, dis_var], how=\"left\", left_index=True, right_index=True, suffixes=('_merged', '_csl'))\n",
    "    \n",
    "    #create a location dataframe to find datapoint\n",
    "    disparity_locations= pd.DataFrame.from_dict(location, orient=\"index\")\n",
    "    \n",
    "    #export to excel\n",
    "    with pd.ExcelWriter('Disparities.xlsx') as writer:\n",
    "        disparity.to_excel(writer, sheet_name=\"Disparities\")\n",
    "        disparity_locations.to_excel(writer, sheet_name=\"Disparity_location\")\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_disparities(df_merged, df_csl) #takes 4 minutes to run this function. modifications for efficiency needed\n",
    "#False indicates no mismatch, True represent disparities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :-\n",
    "- 209752 disparities\n",
    "- variables with disparities: 13\n",
    "- records with disparities: 1,18,886\n",
    "- max disparity on count: Own_Edu- 70084\n",
    "- min disparity on count: city classification, DSTNAME, STATNAME- 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for every column where disparity was found.\n",
    "- greater than, equal to, less than relationships\n",
    "- average change\n",
    "- NOTE: NaN values cannot be compared, hence return false. check if there is actually disparity or simply missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking categorical vars first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframes to compare\n",
    "for col in dis_var:\n",
    "    globals()[col] = []\n",
    "\n",
    "    for i in location.keys():\n",
    "        if col in location[i]:\n",
    "            globals()[col].append(i)\n",
    "        \n",
    "    globals()[col+'_df'] = pd.merge(left= dfm1.loc[globals()[col], col], right=dfc1.loc[globals()[col], col], left_index=True, right_index=True, suffixes=('_merged', '_csl'))\n",
    "    \n",
    "    \n",
    "#Checking disparities\n",
    "display(STATNAME_df)\n",
    "display(DSTNAME_df)\n",
    "display(City_classification_df)\n",
    "display(Focus_region_df)\n",
    "print(\"total null values in merged dataset for these indices\", Focus_region_df['Focus_region_merged'].isna().sum())\n",
    "print(\"total null values in csl dataset for these indices\",Focus_region_df['Focus_region_csl'].isna().sum())\n",
    "\n",
    "display(Marital_status_df)\n",
    "print(\"total null values in merged dataset for these indices\", Marital_status_df['Marital_status_merged'].isna().sum())\n",
    "print(\"total null values in csl dataset for these indices\",Marital_status_df['Marital_status_csl'].isna().sum())\n",
    "\n",
    "display(Occupation_Group_df)\n",
    "print(\"total null values in merged dataset for these indices\", Occupation_Group_df['Occupation_Group_merged'].isna().sum())\n",
    "print(\"total null values in csl dataset for these indices\", Occupation_Group_df['Occupation_Group_csl'].isna().sum())\n",
    "\n",
    "display(Own_Edu_df)\n",
    "print(\"total null values in merged dataset for these indices\", Own_Edu_df['Own_Edu_merged'].isna().sum())\n",
    "print(\"total null values in csl dataset for these indices\", Own_Edu_df['Own_Edu_csl'].isna().sum())\n",
    "\n",
    "display(Own_gender_df)\n",
    "print(\"total null values in merged dataset for these indices\", Own_gender_df['Own_gender_merged'].isna().sum())\n",
    "print(\"total null values in csl dataset for these indices\", Own_gender_df['Own_gender_csl'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thus we observe for above variables, we find disparity because these datapoints contains NaN values. NaN == NaN returns false hence they are counted as disparities. The above can thus be neglected**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking continuous vars\n",
    "\n",
    "- Condition:\n",
    "    - if col1 > col2, categorize as 1\n",
    "    - col1 < col2, categorize as 2\n",
    "    - col1 == col2, categorize as 3\n",
    "- Ideally we should have all as 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afyp\n",
    "conditions = [afyp_df.iloc[:, 0] > afyp_df.iloc[:, 1], afyp_df.iloc[:, 0] < afyp_df.iloc[:, 1], afyp_df.iloc[:, 0] == afyp_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "afyp_df['case'] = np.select(conditions, choices)\n",
    "afyp_df['diff']=100*abs(afyp_df['afyp_merged']-afyp_df['afyp_csl'])/abs(afyp_df['afyp_merged'])\n",
    "display(afyp_df)\n",
    "print(\"value counts for defined cases\", afyp_df['case'].value_counts())\n",
    "display(afyp_df.mean())\n",
    "\n",
    "#owner salary\n",
    "display(Owner_salary_df)\n",
    "print(\"total null values in merged dataset for these indices\", Owner_salary_df['Owner_salary_merged'].isna().sum())\n",
    "print(\"total null values in csl dataset for these indices\", Owner_salary_df['Owner_salary_csl'].isna().sum())\n",
    "\n",
    "#sum_assured\n",
    "conditions = [sum_assured_df.iloc[:, 0] > sum_assured_df.iloc[:, 1], sum_assured_df.iloc[:, 0] < sum_assured_df.iloc[:, 1], sum_assured_df.iloc[:, 0] == sum_assured_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "sum_assured_df['case'] = np.select(conditions, choices)\n",
    "sum_assured_df['diff']=100*abs(sum_assured_df['sum_assured_merged']-sum_assured_df['sum_assured_csl'])/abs(sum_assured_df['sum_assured_merged'])\n",
    "display(sum_assured_df)\n",
    "print(\"value counts for defined cases\", sum_assured_df['case'].value_counts())\n",
    "display(sum_assured_df.mean())\n",
    "x= 100*abs(sum_assured_df['sum_assured_merged'].sum()-sum_assured_df['sum_assured_csl'].sum())/sum_assured_df['sum_assured_merged'].sum()\n",
    "print(\"percent change on sum for sum assured (since 0 as value doesn't allow calculating percentage difference):\", x)\n",
    "\n",
    "#PPT\n",
    "conditions = [PPT_df.iloc[:, 0] > PPT_df.iloc[:, 1], PPT_df.iloc[:, 0] < PPT_df.iloc[:, 1], PPT_df.iloc[:, 0] == PPT_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "PPT_df['case'] = np.select(conditions, choices)\n",
    "PPT_df['diff']=100*abs(PPT_df['PPT_merged']-PPT_df['PPT_csl'])/abs(PPT_df['PPT_merged'])\n",
    "display(PPT_df)\n",
    "print(\"value counts for defined cases\", PPT_df['case'].value_counts())\n",
    "display(PPT_df.mean())\n",
    "\n",
    "#Policy_term\n",
    "conditions = [Policy_term_df.iloc[:, 0] > Policy_term_df.iloc[:, 1], Policy_term_df.iloc[:, 0] < Policy_term_df.iloc[:, 1], Policy_term_df.iloc[:, 0] == Policy_term_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "Policy_term_df['case'] = np.select(conditions, choices)\n",
    "Policy_term_df['diff']=100*abs(Policy_term_df['Policy_term_merged']-Policy_term_df['Policy_term_csl'])/abs(Policy_term_df['Policy_term_merged'])\n",
    "display(Policy_term_df)\n",
    "print(\"value counts for defined cases\", Policy_term_df['case'].value_counts())\n",
    "print(Policy_term_df.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations of disparities:-\n",
    "\n",
    "- All categorical variables disparity arises due to NaN i.e. missing values in both dataframe.\n",
    "    - (reason: np.NaN == np.NaN returns False)\n",
    "    \n",
    "- For all continuous values except Owner_salary, CSL (CustSegList) has higher value than the corresponding datapoint in Merged\n",
    "    - Owner_salary shows mismatch due to NaN values\n",
    "    - afyp: mean percent increase in value is 194.51 %\n",
    "    - sum assured: mean percent increase is Not Defined, due to presence of 0 as a value (which goes in denominator).\n",
    "        - Percent change on sum of values is 131.5 %\n",
    "    - PPT: mean percent increase in value is 92.5 %\n",
    "    - Policy_term: mean percent increase in value is 92.64 %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis: CustSegList is consolidated on merged dataset, rather than subset of merged\n",
    "- CustSegList is consolidated on Policy_owner_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all policy numbers from combined_policy as individual elements\n",
    "l4 = []\n",
    "x= list(df_csl['policy_number'])\n",
    "for rec in df_csl['combined_policy']:\n",
    "    rec = rec.split(',')\n",
    "    for i in rec:\n",
    "        l4.append(int(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicates\n",
    "print(abs(len(set(l4)) - len(l4)))\n",
    "\n",
    "# check if all policy_nos of df_csl are present in combined policy\n",
    "print(\"if all policy_nos of df_csl are present in combined policy\", set(df_csl['policy_number']).issubset(set(l4)))\n",
    "\n",
    "#getting policy nos not in df_csl\n",
    "l5= list(set(l4).difference(set(df_csl['policy_number'])))\n",
    "\n",
    "#check if all these are in df_diff and vice versa\n",
    "print(\"if all policy nos not in df_csl are in df_diff\", set(df_diff['policy_number']).issubset(set(l5)))\n",
    "print(\"if in df_diff are policy nos not in df_csl\", set(l5).issubset(set(df_diff['policy_number'])))\n",
    "\n",
    "#comparing number of records\n",
    "print(len(df_diff))\n",
    "print(len(l5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus all policy numbers in Merged are in [combined policy] in CustSegList  <br />\n",
    "Also, the (number of these records) is same as (duplicate records for policy_owner_number in Merged) <br />\n",
    "Thus concluding CustSegList has data wrapped up on policy_owner_number. <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking continuous vars after grouping by policy_owner_number :\n",
    " - first by sum: afyp, sum_assured\n",
    " - then max: PPT, Policy_term, billing_frequency\n",
    " - then min: RCD, premium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. by sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe grouped by policy owner number from Merged. \n",
    "# assuming afyp and sum assured are sum of combined policies\n",
    "\n",
    "df_grp = df_merged.groupby(['policy_owner_number']).sum()\n",
    "df_grp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare\n",
    "#afyp\n",
    "rafyp_df = pd.merge(left= df_grp[['afyp']], \n",
    "             right=df_csl[df_csl['policy_owner_number'].isin(df_grp.index)][['policy_owner_number','afyp']],\n",
    "             left_index=True, \n",
    "             right_on= 'policy_owner_number', \n",
    "             suffixes=('_mergrp', '_csl'))\n",
    "rafyp_df['afyp_csl'] = rafyp_df['afyp_csl'].apply(np.ceil)\n",
    "rafyp_df['afyp_mergrp'] = rafyp_df['afyp_mergrp'].apply(np.ceil)\n",
    "rafyp_df.drop(['policy_owner_number'], axis = 1, inplace = True)\n",
    "display(rafyp_df)\n",
    "conditions = [rafyp_df.iloc[:, 0] > rafyp_df.iloc[:, 1], \n",
    "              rafyp_df.iloc[:, 0] < rafyp_df.iloc[:, 1], \n",
    "              rafyp_df.iloc[:, 0] == rafyp_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "rafyp_df['case'] = np.select(conditions, choices)\n",
    "rafyp_df['diff']=100*abs(rafyp_df['afyp_mergrp']-rafyp_df['afyp_csl'])/abs(rafyp_df['afyp_mergrp'])\n",
    "print(\"value counts for cases:\", rafyp_df['case'].value_counts())\n",
    "\n",
    "#sum_assured\n",
    "rsum_assured_df = pd.merge(left= df_grp[['sum_assured']], \n",
    "             right=df_csl[df_csl['policy_owner_number'].isin(df_grp.index)][['policy_owner_number','sum_assured']],\n",
    "             left_index=True, \n",
    "             right_on= 'policy_owner_number', \n",
    "             suffixes=('_mergrp', '_csl'))\n",
    "rsum_assured_df['sum_assured_csl'] = rsum_assured_df['sum_assured_csl'].apply(np.ceil)\n",
    "rsum_assured_df['sum_assured_mergrp'] = rsum_assured_df['sum_assured_mergrp'].apply(np.ceil)\n",
    "rsum_assured_df.drop(['policy_owner_number'], axis = 1, inplace = True)\n",
    "display(rsum_assured_df)\n",
    "conditions = [rsum_assured_df.iloc[:, 0] > rsum_assured_df.iloc[:, 1], \n",
    "              rsum_assured_df.iloc[:, 0] < rsum_assured_df.iloc[:, 1], \n",
    "              rsum_assured_df.iloc[:, 0] == rsum_assured_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "rsum_assured_df['case'] = np.select(conditions, choices)\n",
    "print(\"value counts for cases:\", rsum_assured_df['case'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. by max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe grouped by policy owner number from Merged. \n",
    "# assuming PPT and Policy_term is taken as bigger of the two values in combined policy\n",
    "\n",
    "y =['policy_owner_number',\n",
    " 'policy_number',\n",
    " 'PPT','Policy_term', 'billing_frequency']\n",
    "df_grp = (df_merged.loc[:, y]).groupby(['policy_owner_number']).max()\n",
    "df_grp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PPT\n",
    "rPPT_df = pd.merge(left= df_grp[['PPT']], \n",
    "             right=df_csl[df_csl['policy_owner_number'].isin(df_grp.index)][['policy_owner_number','PPT']],\n",
    "             left_index=True, \n",
    "             right_on= 'policy_owner_number', \n",
    "             suffixes=('_mergrp', '_csl'))\n",
    "rPPT_df['PPT_csl'] = rPPT_df['PPT_csl'].apply(np.ceil)\n",
    "rPPT_df['PPT_mergrp'] = rPPT_df['PPT_mergrp'].apply(np.ceil)\n",
    "rPPT_df.drop(['policy_owner_number'], axis = 1, inplace = True)\n",
    "display(rPPT_df)\n",
    "conditions = [rPPT_df.iloc[:, 0] > rPPT_df.iloc[:, 1], rPPT_df.iloc[:, 0] < rPPT_df.iloc[:, 1], rPPT_df.iloc[:, 0] == rPPT_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "rPPT_df['case'] = np.select(conditions, choices)\n",
    "print(\"value counts for cases:\", rPPT_df['case'].value_counts())\n",
    "\n",
    "#Policy_term\n",
    "rPolicy_term_df = pd.merge(left= df_grp[['Policy_term']], \n",
    "             right=df_csl[df_csl['policy_owner_number'].isin(df_grp.index)][['policy_owner_number','Policy_term']],\n",
    "             left_index=True, \n",
    "             right_on= 'policy_owner_number', \n",
    "             suffixes=('_mergrp', '_csl'))\n",
    "rPolicy_term_df['Policy_term_csl'] = rPolicy_term_df['Policy_term_csl'].apply(np.ceil)\n",
    "rPolicy_term_df['Policy_term_mergrp'] = rPolicy_term_df['Policy_term_mergrp'].apply(np.ceil)\n",
    "rPolicy_term_df.drop(['policy_owner_number'], axis = 1, inplace = True)\n",
    "display(rPolicy_term_df)\n",
    "conditions = [rPolicy_term_df.iloc[:, 0] > rPolicy_term_df.iloc[:, 1], rPolicy_term_df.iloc[:, 0] < rPolicy_term_df.iloc[:, 1], rPolicy_term_df.iloc[:, 0] == rPolicy_term_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "rPolicy_term_df['case'] = np.select(conditions, choices)\n",
    "print(\"value counts for cases:\", rPolicy_term_df['case'].value_counts())\n",
    "\n",
    "#billing_frequency\n",
    "rbilling_frequency_df = pd.merge(left= df_grp[['billing_frequency']], \n",
    "             right=df_csl[df_csl['policy_owner_number'].isin(df_grp.index)][['policy_owner_number','billing_frequency']],\n",
    "             left_index=True, \n",
    "             right_on= 'policy_owner_number', \n",
    "             suffixes=('_mergrp', '_csl'))\n",
    "rbilling_frequency_df['billing_frequency_csl'] = rbilling_frequency_df['billing_frequency_csl'].apply(np.ceil)\n",
    "rbilling_frequency_df['billing_frequency_mergrp'] = rbilling_frequency_df['billing_frequency_mergrp'].apply(np.ceil)\n",
    "rbilling_frequency_df.drop(['policy_owner_number'], axis = 1, inplace = True)\n",
    "display(rbilling_frequency_df)\n",
    "conditions = [rbilling_frequency_df.iloc[:, 0] > rbilling_frequency_df.iloc[:, 1], rbilling_frequency_df.iloc[:, 0] < rbilling_frequency_df.iloc[:, 1], rbilling_frequency_df.iloc[:, 0] == rbilling_frequency_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "rbilling_frequency_df['case'] = np.select(conditions, choices)\n",
    "print(\"value counts for cases:\", rbilling_frequency_df['case'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. by min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe grouped by policy owner number from Merged. \n",
    "# assuming PPT and Policy_term is taken as bigger of the two values in combined policy\n",
    "\n",
    "y =['policy_owner_number',\n",
    " 'policy_number',\n",
    " 'RCD', 'premium']\n",
    "df_grp = (df_merged.loc[:, y]).groupby(['policy_owner_number']).min()\n",
    "df_grp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RCD\n",
    "rRCD_df = pd.merge(left= df_grp[['RCD']], \n",
    "             right=df_csl[df_csl['policy_owner_number'].isin(df_grp.index)][['policy_owner_number','RCD']],\n",
    "             left_index=True, \n",
    "             right_on= 'policy_owner_number', \n",
    "             suffixes=('_mergrp', '_csl'))\n",
    "\n",
    "rRCD_df.drop(['policy_owner_number'], axis = 1, inplace = True)\n",
    "display(rRCD_df)\n",
    "conditions = [rRCD_df.iloc[:, 0] > rRCD_df.iloc[:, 1], rRCD_df.iloc[:, 0] < rRCD_df.iloc[:, 1], rRCD_df.iloc[:, 0] == rRCD_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "rRCD_df['case'] = np.select(conditions, choices)\n",
    "print(\"value counts for cases:\", rRCD_df['case'].value_counts())\n",
    "\n",
    "#premium\n",
    "rpremium_df = pd.merge(left= df_grp[['premium']], \n",
    "             right=df_csl[df_csl['policy_owner_number'].isin(df_grp.index)][['policy_owner_number','premium']],\n",
    "             left_index=True, \n",
    "             right_on= 'policy_owner_number', \n",
    "             suffixes=('_mergrp', '_csl'))\n",
    "rpremium_df['premium_csl'] = rpremium_df['premium_csl'].apply(np.ceil)\n",
    "rpremium_df['premium_mergrp'] = rpremium_df['premium_mergrp'].apply(np.ceil)\n",
    "rpremium_df.drop(['policy_owner_number'], axis = 1, inplace = True)\n",
    "display(rpremium_df)\n",
    "conditions = [rpremium_df.iloc[:, 0] > rpremium_df.iloc[:, 1], rpremium_df.iloc[:, 0] < rpremium_df.iloc[:, 1], rpremium_df.iloc[:, 0] == rpremium_df.iloc[:, 1]]\n",
    "choices = [1,2,3]\n",
    "rpremium_df['case'] = np.select(conditions, choices)\n",
    "print(\"value counts for cases:\", rpremium_df['case'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Observations:\n",
    "    - Thus, Merged dataset is wrapped up data of CustSegList on 'policy_owner_number'\n",
    "    - Rather than a simple groupby, this is done by assessing individual columns differently\n",
    "    - Categorical Variables:\n",
    "        - these contain Identity Information which stays same across both datasets\n",
    "     - Continuous Variables :\n",
    "         - Any Identity Information stays same, eg. Owner salary\n",
    "         - Variables related to Insurance policies change: \n",
    "             - sum_assured and afyp are added. \n",
    "             - PPT, policy term, billing frequency, max of two values is chosen\n",
    "             - premium, smaller of the values is chosen for majority rows\n",
    "     - DateTime:\n",
    "         - DOB stays same\n",
    "         - RCD, smaller of the values is chosen for majority rows\n",
    "         - RCD for range for both datasets: 2013-01-01 to 2017-12-03\n",
    "  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
