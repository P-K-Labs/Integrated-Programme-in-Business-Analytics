{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose and Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose:-\n",
    "\n",
    "- To fit a model on 'Dataset_model'\n",
    "- Dataset: created by isolating first policy bought by a customer (using policy_owner_number and RCD)\n",
    "\n",
    "### Steps:-\n",
    "- Oversampling and 3 fold Cross validation performed\n",
    "- Iteration1: \n",
    "    - train test split to check feature importance\n",
    "    - K fold validation to check parameters\n",
    "- Iteration 2:\n",
    "    - K fold with select features\n",
    "- Iteration 3:\n",
    "    - Fine tuning: Voluntarily increasing Recall to lower false negatives.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset used Dataset_model: Merged cleaned, continuous variables imputed with mean, rest recordes with NaN dropped. then isolate records for first RCD, groupedby policy_owner_number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "dataset = pd.read_csv(\"Dataset_model.csv\")\n",
    "\n",
    "#convert RCD to datetime\n",
    "dataset[\"RCD\"] = pd.to_datetime(dataset[\"RCD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Stats for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tweak to show full values in describe instead of in exp terms\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "np.set_printoptions(suppress = True)\n",
    "#describe and info\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(dataset.head())\n",
    "    display(dataset.describe())\n",
    "    display(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping unneeded variables\n",
    "- Dropping columns\n",
    "    - 'Own_Education': keeping Own_Edu\n",
    "    - 'own_occupation: keeping Occ_Profile, Occupation_Group, Occupation\n",
    "    - 'policy_number', 'policy_owner_number': Identifiers\n",
    "    - 'RCD': Datetime\n",
    "    - 'Freq': used to obtain target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['Own_Education', 'own_occupation',\n",
    "              'policy_number', 'policy_owner_number', \n",
    "              'RCD', \n",
    "              'Freq'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert floats to int\n",
    "floatlist = list(dataset.select_dtypes('float').columns)\n",
    "\n",
    "for col in floatlist:\n",
    "    dataset.loc[:,col] = dataset.loc[:,col].apply(np.ceil)\n",
    "    if dataset.loc[:,col].isna().sum() == 0:\n",
    "        dataset[col] = dataset[col].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelencode for categorical vars\n",
    "\n",
    "#create list of categorical vars\n",
    "catcolsm = list(dataset.select_dtypes('object').columns)\n",
    "\n",
    "#encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#function to encode\n",
    "def labelencode(data, col):\n",
    "    nonulls=np.array(data.dropna())\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    impute_ordinal = le.fit_transform(impute_reshape)\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    globals()[col+'_map'] = dict(zip(range(len(le.classes_)), le.classes_))\n",
    "    return data\n",
    "\n",
    "#labelencode\n",
    "for col in catcolsm:\n",
    "    labelencode(dataset[col], col)\n",
    "    \n",
    "#review results\n",
    "display(dataset.head())\n",
    "\n",
    "#display dictionaries created.\n",
    "for col in catcolsm:\n",
    "    display(globals()[col+'_map'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert object dtype to category dtype\n",
    "for col in catcolsm:\n",
    "    dataset[col] = dataset[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seperating target variable\n",
    "X1=dataset.drop(['target'],axis=1)\n",
    "y1=dataset[['target']]\n",
    "X1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "oversample = RandomOverSampler(sampling_strategy=0.5, random_state= 211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversample fit\n",
    "X1_over, y1_over = oversample.fit_resample(X1,y1)\n",
    "\n",
    "#check Counts after oversampling\n",
    "print(\"Before oversampling:\", Counter(y1['target']))\n",
    "print(\"Before oversampling:\", Counter(y1_over['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier: Iteration 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. dataset split, feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model1\n",
    "#took 5-6 mins to run\n",
    "\n",
    "x_train, x_test, y_train,  y_test = train_test_split(X1_over, y1_over, test_size=0.2, random_state=0)\n",
    "rf=RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking feature importances again using RandomForestClassifier\n",
    "imp_dict = {}\n",
    "for i,j in list(zip(x_test, rf.feature_importances_)):\n",
    "    imp_dict[i] = j\n",
    "imp_dict = {k:v for k,v in sorted(imp_dict.items(), key =lambda item: item[1])}\n",
    "\n",
    "plt.bar([x for x in range(len(imp_dict.keys()))], [y for y in imp_dict.values()])\n",
    "plt.show()\n",
    "\n",
    "display(imp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imp variables from RandomForestClassifier** (> or close to 0.05)\n",
    "- premium, afyp, sum_assured, Owner_salary, city, CUST_prod_cat, DSTNAME, STATNAME, age: having high importance\n",
    "- Own_Edu, Occupation: moderate importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#randomforestclassifier\n",
    "clf1 = RandomForestClassifier(n_estimators = 50, \n",
    "                              random_state = 12,\n",
    "                             max_features = 10,\n",
    "                             max_depth = 15,\n",
    "                             min_samples_leaf = 2)\n",
    "\n",
    "#Kfolds\n",
    "cv1 = StratifiedKFold(n_splits = 3, random_state=123, shuffle = True)\n",
    "\n",
    "#list and Dataframe for different metrics\n",
    "df_score1 = pd.DataFrame(columns = ['loop', 'metric', 'train', 'test'])\n",
    "\n",
    "loop1, metric1, training1, testing1, confusion_matrix_all1 = [], [], [], [], []\n",
    "auc1, fprs1, tprs1 = [], [], []\n",
    "\n",
    "#Fit model\n",
    "for (train,test), i in zip(cv1.split(X1_over,y1_over), range(3)):\n",
    "    #fit\n",
    "    X_train1, X_test1, y_train1, y_test1 = X1_over.iloc[train], X1_over.iloc[test], y1_over.iloc[train], y1_over.iloc[test]\n",
    "    clf1.fit(X_train1, y_train1)\n",
    "    \n",
    "    #predict\n",
    "    y_predict_train1 = clf1.predict(X_train1)\n",
    "    y_predict_test1 = clf1.predict(X_test1)\n",
    "    \n",
    "    #scores\n",
    "    #accuracy\n",
    "    loop1.append(i)\n",
    "    metric1.append('accuracy')\n",
    "    training1.append(metrics.accuracy_score(y_train1, y_predict_train1))\n",
    "    testing1.append(metrics.accuracy_score(y_test1, y_predict_test1))\n",
    "    \n",
    "    #f1\n",
    "    loop1.append(i)\n",
    "    metric1.append('f1')\n",
    "    training1.append(metrics.f1_score(y_train1, y_predict_train1))\n",
    "    testing1.append(metrics.f1_score(y_test1, y_predict_test1))\n",
    "    \n",
    "    #precision\n",
    "    loop1.append(i)\n",
    "    metric1.append('precision')\n",
    "    training1.append(metrics.precision_score(y_train1, y_predict_train1))\n",
    "    testing1.append(metrics.precision_score(y_test1, y_predict_test1))\n",
    "    \n",
    "    #recall\n",
    "    loop1.append(i)\n",
    "    metric1.append('recall')\n",
    "    training1.append(metrics.recall_score(y_train1, y_predict_train1))\n",
    "    testing1.append(metrics.recall_score(y_test1, y_predict_test1))\n",
    "    \n",
    "    #confusion matrix on test data\n",
    "    confusion_matrix_all1.append(metrics.confusion_matrix(y_test1, y_predict_test1))\n",
    "    \n",
    "    #AUC, ROC for test data\n",
    "    y_predict_prob1 = clf1.predict_proba(X_test1)\n",
    "    auc_score1 = metrics.roc_auc_score(y_test1, y_predict_prob1[:,1])\n",
    "    fpr1, tpr1, threshold1 = metrics.roc_curve(y_test1, y_predict_prob1[:, 1])\n",
    "    fprs1.append(fpr1)\n",
    "    tprs1.append(tpr1)\n",
    "    auc1.append(auc_score1)\n",
    "    #fprs and tprs contain 3 arrays now\n",
    "    \n",
    "#append Dataframe and view results\n",
    "df_score1['loop'] = loop1\n",
    "df_score1['metric'] = metric1\n",
    "df_score1['train'] = training1\n",
    "df_score1['test'] = testing1\n",
    "\n",
    "display(df_score1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Rocs\n",
    "for i in range(3): \n",
    "    plt.plot(fprs1[i], tprs1[i], linestyle = '--', label = 'fold %d (AUROC = %0.3f)' %(i,auc1[i]))\n",
    "#title\n",
    "plt.title('ROC plot')\n",
    "#labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#legend\n",
    "plt.legend()\n",
    "#show plot\n",
    "plt.show()\n",
    "\n",
    "print('mean accuracy on test dataset:', df_score1[df_score1['metric'] == 'accuracy']['test'].mean())\n",
    "print('mean f1 score on test dataset:', df_score1[df_score1['metric'] == 'f1']['test'].mean())\n",
    "print('mean precision on test dataset:', df_score1[df_score1['metric'] == 'precision']['test'].mean())\n",
    "print('mean recall on test dataset:', df_score1[df_score1['metric'] == 'recall']['test'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = np.mean(confusion_matrix_all1, axis = 0)\n",
    "total1 = np.sum(cm1)\n",
    "cm1 = pd.DataFrame(cm1, columns = [0,1] , index = [0,1])\n",
    "display(cm1)\n",
    "\n",
    "#percentage\n",
    "print('percentage:')\n",
    "display(cm1/total1*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier: Iteration 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select important features \n",
    "1. feature importance close to 0.05 and above\n",
    "2. Making sure atleast one feature from every categorization (mentioned in EDA: Amounts, etc) is present. Choosing variable with highest feature importance if not present in selection made in point 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating target variable\n",
    "X2=X1_over[['premium', 'afyp', 'sum_assured', 'Owner_salary', 'city', 'CUST_prod_cat', 'DSTNAME', 'STATNAME', 'age',\n",
    "           'Own_Edu', 'Occupation',\n",
    "            'City_classification',\n",
    "            'contract_type',\n",
    "            'channel_flag',\n",
    "            'Product_Club_Manual',\n",
    "           'Policy_term']]\n",
    "y2=y1_over[['target']]\n",
    "X2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#randomforestclassifier\n",
    "clf2 = RandomForestClassifier(n_estimators = 50, \n",
    "                              random_state = 13,\n",
    "                             max_features = 10,\n",
    "                             max_depth = 15,\n",
    "                             min_samples_leaf = 2)\n",
    "\n",
    "#Kfolds\n",
    "cv2 = StratifiedKFold(n_splits = 3, random_state=124, shuffle = True)\n",
    "\n",
    "#list and Dataframe for different metrics\n",
    "df_score2 = pd.DataFrame(columns = ['loop', 'metric', 'train', 'test'])\n",
    "\n",
    "loop2, metric2, training2, testing2, confusion_matrix_all2 = [], [], [], [], []\n",
    "auc2, fprs2, tprs2 = [], [], []\n",
    "\n",
    "#Fit model\n",
    "for (train,test), i in zip(cv2.split(X2,y2), range(3)):\n",
    "    #fit\n",
    "    X_train2, X_test2, y_train2, y_test2 = X2.iloc[train], X2.iloc[test], y2.iloc[train], y2.iloc[test]\n",
    "    clf2.fit(X_train2, y_train2)\n",
    "    \n",
    "    #predict\n",
    "    y_predict_train2 = clf2.predict(X_train2)\n",
    "    y_predict_test2 = clf2.predict(X_test2)\n",
    "    \n",
    "    #scores\n",
    "    #accuracy\n",
    "    loop2.append(i)\n",
    "    metric2.append('accuracy')\n",
    "    training2.append(metrics.accuracy_score(y_train2, y_predict_train2))\n",
    "    testing2.append(metrics.accuracy_score(y_test2, y_predict_test2))\n",
    "    \n",
    "    #f1\n",
    "    loop2.append(i)\n",
    "    metric2.append('f1')\n",
    "    training2.append(metrics.f1_score(y_train2, y_predict_train2))\n",
    "    testing2.append(metrics.f1_score(y_test2, y_predict_test2))\n",
    "    \n",
    "    #precision\n",
    "    loop2.append(i)\n",
    "    metric2.append('precision')\n",
    "    training2.append(metrics.precision_score(y_train2, y_predict_train2))\n",
    "    testing2.append(metrics.precision_score(y_test2, y_predict_test2))\n",
    "    \n",
    "    #recall\n",
    "    loop2.append(i)\n",
    "    metric2.append('recall')\n",
    "    training2.append(metrics.recall_score(y_train2, y_predict_train2))\n",
    "    testing2.append(metrics.recall_score(y_test2, y_predict_test2))\n",
    "    \n",
    "    #confusion matrix on test data\n",
    "    confusion_matrix_all2.append(metrics.confusion_matrix(y_test2, y_predict_test2))\n",
    "    \n",
    "    #auc2, ROC for test data\n",
    "    y_predict_prob2 = clf2.predict_proba(X_test2)\n",
    "    auc_score2 = metrics.roc_auc_score(y_test2, y_predict_prob2[:,1])\n",
    "    fpr2, tpr2, threshold2 = metrics.roc_curve(y_test2, y_predict_prob2[:, 1])\n",
    "    fprs2.append(fpr2)\n",
    "    tprs2.append(tpr2)\n",
    "    auc2.append(auc_score2)\n",
    "    #fprs2 and tprs2 contain 3 arrays now\n",
    "    \n",
    "#append Dataframe and view results\n",
    "df_score2['loop'] = loop2\n",
    "df_score2['metric'] = metric2\n",
    "df_score2['train'] = training2\n",
    "df_score2['test'] = testing2\n",
    "\n",
    "display(df_score2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Rocs\n",
    "for i in range(3): \n",
    "    plt.plot(fprs2[i], tprs2[i], linestyle = '--', label = 'fold %d (AUROC = %0.3f)' % (i,auc2[i]))\n",
    "#title\n",
    "plt.title('ROC plot')\n",
    "#labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#legend\n",
    "plt.legend()\n",
    "#show plot\n",
    "plt.show()\n",
    "\n",
    "print('mean accuracy on test dataset:', df_score2[df_score2['metric'] == 'accuracy']['test'].mean())\n",
    "print('mean f1 score on test dataset:', df_score2[df_score2['metric'] == 'f1']['test'].mean())\n",
    "print('mean precision on test dataset:', df_score2[df_score2['metric'] == 'precision']['test'].mean())\n",
    "print('mean recall on test dataset:', df_score2[df_score2['metric'] == 'recall']['test'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm2 = np.mean(confusion_matrix_all2, axis = 0)\n",
    "total2 = np.sum(cm2)\n",
    "cm2 = pd.DataFrame(cm2, columns = [0,1] , index = [0,1])\n",
    "display(cm2)\n",
    "\n",
    "#percentage\n",
    "print('percentage:')\n",
    "display((cm2/total2*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with new threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set threshold\n",
    "\n",
    "threshold = 0.25\n",
    "\n",
    "df_score3 = pd.DataFrame(columns = ['loop', 'metric', 'test'])\n",
    "loop3, metric3, testing3, confusion_matrix_all3 = [],[],[], []\n",
    "#For every fold in Kfold cv2\n",
    "#create column predicted based on threshold\n",
    "for (train,test), i in zip(cv2.split(X2,y2), range(3)):\n",
    "    \n",
    "    #fitting and getting predicted values using iteration 2 splits\n",
    "    X_train2, X_test2, y_train2, y_test2 = X2.iloc[train], X2.iloc[test], y2.iloc[train], y2.iloc[test]\n",
    "    clf2.fit(X_train2, y_train2)\n",
    "    y_predict_prob2 = clf2.predict_proba(X_test2)\n",
    "    predicted = (y_predict_prob2[:,1] >= threshold).astype('int')\n",
    "    \n",
    "    #accuracy\n",
    "    loop3.append(i)\n",
    "    metric3.append('accuracy')\n",
    "    testing3.append(metrics.accuracy_score(y_test2, predicted))\n",
    "    \n",
    "    #f1\n",
    "    loop3.append(i)\n",
    "    metric3.append('f1')\n",
    "    testing3.append(metrics.f1_score(y_test2, predicted))\n",
    "    \n",
    "    #precision\n",
    "    loop3.append(i)\n",
    "    metric3.append('precision')\n",
    "    testing3.append(metrics.precision_score(y_test2, predicted))\n",
    "    \n",
    "    #recall\n",
    "    loop3.append(i)\n",
    "    metric3.append('recall')\n",
    "    testing3.append(metrics.recall_score(y_test2, predicted))\n",
    "    \n",
    "    #confusion matrix on test data\n",
    "    confusion_matrix_all3.append(metrics.confusion_matrix(y_test2, predicted))\n",
    "\n",
    "df_score3['loop'] = loop3\n",
    "df_score3['metric'] = metric3\n",
    "df_score3['test'] = testing3\n",
    "\n",
    "display(df_score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean accuracy on test dataset:', df_score3[df_score3['metric'] == 'accuracy']['test'].mean())\n",
    "print('mean f1 score on test dataset:', df_score3[df_score3['metric'] == 'f1']['test'].mean())\n",
    "print('mean precision on test dataset:', df_score3[df_score3['metric'] == 'precision']['test'].mean())\n",
    "print('mean recall on test dataset:', df_score3[df_score3['metric'] == 'recall']['test'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm3 = np.mean(confusion_matrix_all3, axis = 0)\n",
    "total3 = np.sum(cm3)\n",
    "cm3 = pd.DataFrame(cm3, columns = [0,1] , index = [0,1])\n",
    "display(cm3)\n",
    "\n",
    "#percentage\n",
    "print('percentage:')\n",
    "display((cm3/total3*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
